# 具体的数学推导优化方向

## 相同点

毫米波信道，数字预编码矩阵，接收信号中有一部分因子是直连路径

## 不同点

**UPA阵列**相乘，**路径增益系数复杂化**，引入了**信道误差因子（服从复高斯分布**）；
原和速率目标函数，近似函数推导中将期望操作引入log函数内部；现使用MSE函数。
原为单信道，现为级联信道，且RIS反射幅度涉及**PDA（相位依赖的幅度）耦合模型**；
理想RIS相位需要**量化** *（这个有点棘手）*，量化之后引入**特定分布**的误差因子；

## 分析角度

- 《Deep Reinforcement Learning Based Beamforming  Codebook Design for RIS-aided mmWave Systems》中，使用模拟beamforming方案，从而将BS端、RIS端beamforming向量整合，**整合之后对变量进行化简合并**。这里BS、RIS端波束角度都取值于量化集合，且RIS反射向量取值于码本，对于这个复杂的离散优化问题，作者没有讨论，而是使用强化学习智能体解决。
- **todo Second**《Beam Training and Allocation for Multiuser  Millimeter Wave Massive MIMO Systems》中，将**波束训练与混合预编码结构结合**，根据处理得到的接受信号强度信息，选择模拟域beam forming向量，而接收信号强度信息本身，就是实际的信道增益系数。但是**缺少RIS辅助系统下的波束训练+混合预编码信息**，无法确定上述方法是否合适。
- 《Worst-case MSE Minimization for RIS-assisted  mmWave MU-MISO Systems with Hardware  Impairments and Imperfect CSI》中，对MSE计算式简化，用作强化学习智能体的奖励函数。如果我引入波束训练框架下的混合预编码结构应用，是否依然按照 *统计信道* **重新推导MSE简化式**呢？
- **todo First** 波束训练框架下，**信道误差模型**怎么表示？
- 强化学习的一大问题是缺乏有意义的奖励反馈，特别是输入H这种抽象信息、输出动作空间如数字域beamforming向量这种情况时，奖励空间极为稀疏。课程学习是一个有希望的解决方向，为此准备，可以**分析如何将抽象的beam forming分解为几个阶段的小任务，或是从简化的奖励函数到复杂化的奖励函数。**
- *额外小点*，上行波束训练框架下，按照码本波束发射，用户端应用低精度ADC会对接收信号强度、信道估计结果造成什么误差影响呢？此外，RIS系统下，上行波束训练方法怎么进行呢?
